---
title: "L12 — Text Analysis (02)"
subtitle: "TF-IDF & Topic Modeling"
author: 'Daniel Stoxreiter'
output:
  html_document:
    df_print: paged
    toc: true
    highlight: tango
    number_sections: yes
---

**NB:** The worksheet has beed developed and prepared by Maxim Romanov for the course "R for Historical Research" (U Vienna, Spring 2019).

# Goals

- introduce text analysis approaches:
  - tf-idf (term frequency–inverse document frequency)
  - topic modeling

# Preliminaries

## Libraries

The following are the libraries that we will need for this section. Install those that you do not have yet.

```{r message=FALSE}
#install.packages("ggplot2", "LDAvis", "readr", "slam", "stringr", "tictoc", "tidytext", "tidyverse", "tm", "topicmodels")

# general
library(ggplot2)

# text analysis specific
library(readr)
library(slam)
library(stringr)
library(tidytext)
library(tidyverse)
library(tm)
library(topicmodels)

# extra
library(tictoc) # to time operations
```


# The Dispatch Data

```{r}
d1864 <- read.delim("./dispatch/dispatch_1864_filtered.tsv", encoding="UTF-8", header=TRUE, quote="", stringsAsFactors = FALSE)
d1864$date <- as.Date(d1864$date, format="%Y-%m-%d")
head(d1864)
```

Let's remove low freq items:

```{r}
d1864_lowFreq <- d1864 %>%
  unnest_tokens(word, text) %>%
  count(word, sort=TRUE)

summary(d1864_lowFreq)
```

```{r}
lowFreq <- d1864_lowFreq %>%
  filter(n < 4)
summary(lowFreq)
```

Most of these low-frequency items are typos:

```{r}
lowFreq
```

We can `anti-join` our corpus with `lowFreq`, which will remove them:

```{r}
d1864_clean <- d1864 %>%
  filter(type != "ad_blank") %>%
  filter(type != "ad-blank") #%>% filter(type != "advert")

d1864_clean <- d1864_clean %>%
  unnest_tokens(word, text) %>%
  anti_join(lowFreq, by="word") %>%
  group_by(id) %>%
  count(word, sort=TRUE)

# unfiltered:      2,815,144
# filtered (>3):   2,749,078
```

Additionally, we need to remove `stop words`, but first we need to identify them.

```{r}
d1864_clean_FL <- d1864_clean %>%
  group_by(word) %>%
  summarize(freq=sum(n)) %>%
  arrange(desc(freq))

d1864_clean_FL
```

To make things faster, you can remove top 50, 100, 150, 200 most frequent words, but this is a rather brutal way. Ideally, you want to curate your own stop word list that will be tuned to your texts. Below, I have taken top 500 words and manually removed everything that was meaningful (or, better, what I *considered* meaningful). Additionally, there are also NLP procedures that are designed to lemmatize words (i.e., reduce all words to their dictionary forms) and also do part-of-speech tagging, which allows to remove words categorically (for example, keeping only nouns, adjectives and verbs). 

```{r}
word <- c("the", "of", "and", "to", "in", "a", "that", "for", "on", "was", "is", "at", "be", "by", "from", "his", "he", "it", "with", "as", "this", "will", "which", "have", "or", "are", "they", "their", "not", "were", "been", "has", "our", "we", "all", "but", "one", "had", "who", "an", "no", "i", "them", "about", "him", "two", "upon", "may", "there", "any", "some", "so", "men", "when", "if", "day", "her", "under", "would", "c", "such", "made", "up", "last", "j", "time", "years", "other", "into", "said", "new", "very", "five", "after", "out", "these", "shall", "my", "w", "more", "its", "now", "before", "three", "m", "than", "h", "o'clock", "old", "being", "left", "can", "s", "man", "only", "same", "act", "first", "between", "above", "she", "you", "place", "following", "do", "per", "every", "most", "near", "us", "good", "should", "having", "great", "also", "over", "r", "could", "twenty", "people", "those", "e", "without", "four", "received", "p", "then", "what", "well", "where", "must", "says", "g", "large", "against", "back", "000", "through", "b", "off", "few", "me", "sent", "while", "make", "number", "many", "much", "give", "1", "six", "down", "several", "high", "since", "little", "during", "away", "until", "each", "5", "year", "present", "own", "t", "here", "d", "found", "reported", "2", "right", "given", "age", "your", "way", "side", "did", "part", "long", "next", "fifty", "another", "1st", "whole", "10", "still", "among", "3", "within", "get", "named", "f", "l", "himself", "ten", "both", "nothing", "again", "n", "thirty", "eight", "took", "never", "came", "called", "small", "passed", "just", "brought", "4", "further", "yet", "half", "far", "held", "soon", "main", "8", "second", "however", "say", "heavy", "thus", "hereby", "even", "ran", "come", "whom", "like", "cannot", "head", "ever", "themselves", "put", "12", "cause", "known", "7", "go", "6", "once", "therefore", "thursday", "full", "apply", "see", "though", "seven", "tuesday", "11", "done", "whose", "let", "how", "making", "immediately", "forty", "early", "wednesday", "either", "too", "amount", "fact", "heard", "receive", "short", "less", "100", "know", "might", "except", "supposed", "others", "doubt", "set", "works") 

sWordsDF <- data.frame(word)

d1864_clean_minusSW <- d1864_clean %>%
  anti_join(sWordsDF, by="word")
```

```{r}
dim(d1864_clean)
```

```{r}
dim(d1864_clean_minusSW)
```

# TF-IDF

From Wikipedia: In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf. Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification. One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.

```{r}
df_TF_IDF <- d1864_clean_minusSW %>% # d1864_clean, d1864_clean_minusSW
  bind_tf_idf(word, id, n) %>%
  arrange(desc(tf_idf)) %>%
  ungroup

df_TF_IDF
```

```{r}
articleID = "1864-06-30_orders_56"
filter(df_TF_IDF, id==articleID) %>%
  arrange(desc(tf_idf))
```

```{r}
d1864$text[d1864$id==articleID]
```

# Topic Modeling

We can start with our preprocessed variable `d1864_clean`, which is essentially a cumulative frequency list for all articles.

```{r}
library(tm)
library(topicmodels)
library(tictoc)
```

```{r}
head(d1864_clean_minusSW)
```

```{r}
d1864_dm <- d1864_clean_minusSW %>%
  cast_dtm(id, word, n)
d1864_dm
```

Training a model. **NB**: `eval=FALSE` setting will prevent from running this chunk when you `Knit` the notebook; but you can still execute it within the notebook, when you run chunks individually

```{r eval=FALSE}
'tic()
# k of LDA is the number of topics used for the corpus
d1864_lda <- LDA(d1864_dm, k = 4, control = list(seed = 1234))
d1864_lda

toc()'

#A LDA_VEM topic model with 2 topics.
#35.962 sec elapsed

#A LDA_VEM topic model with 4 topics.
#72.545 sec elapsed
```

**Do not run this!**

```{r eval=FALSE}
'tic()

kVal <- 25
d1864_lda_better <- LDA(d1864_dm, k=kVal, control=list(seed=1234))

toc()'

#A LDA_VEM topic model with 20 topics.
#1261.087 sec elapsed (21 minutes)

#A LDA_VEM topic model with 25 topics.
#1112.262 sec elapsed (18 minutes)

```

Save/load the model, so that there is no need to regenerate it every time.

```{r}

#d1864_lda_vem_25t_better <- d1864_lda_better
#save(d1864_lda_vem_25t_better, file="d1864_lda_vem_25t_better.rda")
#load(file="d1864_lda_vem_25t_better.rda")

```

```{r}
load(file="d1864_lda_vem_25t_better.rda")
lda_model <- d1864_lda_vem_25t_better
corpus <- d1864
```

From this point on, the code should simply run --- if you rename your own model produced above to `lda_model`.

## Per-topic-per-word probabilities

```{r}
lda_model_better_topic_term_prob <- tidy(lda_model, matrix="beta")

lda_model_better_topic_term_prob %>%
  filter(term == "bonds") %>%
  arrange(desc(beta))
```

**NB:** This step may throw an error. The error seems a bit cryptic, but restarting R (without saving workspace), seems to help. (*beta* stands for term-per-topic probability).

```{r}

top_terms <- lda_model_better_topic_term_prob %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

head(top_terms)

```

```{r fig.height=5, fig.width=9}

library(ggplot2)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend=FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip()

```

Topic-per-document probabilities: this object will tell us to which topics documents belong (and to what extent):
(*gamma* stands for topic-per-document probability).

```{r}
lda_model_topic_doc_prob <- tidy(lda_model, matrix="gamma")
lda_model_topic_doc_prob
```

(@) Pick a document and print out topics it belongs to (from the most prominent to less prominent). (Hint: use the obejct we just created > filter > arrange).

```{r}

# your code here

```

Top N documents per topic: this will create an ojbect with top N documents per each topic.

```{r}

N = 10

top_docs <- lda_model_topic_doc_prob %>%
  group_by(topic) %>%
  top_n(N, gamma) %>%
  ungroup() %>%
  arrange(topic, -gamma)

top_docs
```

Now that we have IDs of representative documents, we can check those documents one by one, but le'ts do somthing else first: topic-title function—it is not really necessary, but it will combine together topic number (from the model) and its top words, which can be used for graphs.

```{r}
topicVar <- function(top, topTerms){
  topicVar <- topTerms %>%
    filter(topic == top) %>%
    arrange(-beta)
  vals = paste(topicVar$term, collapse=", ")
  as.String(paste(c("Topic ", top, ": ", vals), collapse=""))
}
```


```{r}
topicNum = 8 
idTest   = "1864-08-31_orders_74"

topicVar(topicNum, top_terms)
as.String(d1864[d1864$id==idTest, ]["text"])
```

##  Topics over time:

```{r}

corpus_light <- corpus %>%
  select(-header, -text)

lda_model_topics <- lda_model_topic_doc_prob %>%
  rename(id=document) %>%
  left_join(corpus_light, by="id") %>%
  group_by(topic, date) %>%
  summarize(freq=sum(gamma))

lda_model_topics

```

Now, we can plot topic distribution over time:

```{r fig.height=4, fig.width=9}
topicVal = 8

lda_model_topics_final <- lda_model_topics %>%
  filter(topic == topicVal)

plot(x=lda_model_topics_final$date, y=lda_model_topics_final$freq, type="l", lty=3, lwd=1,
     main=topicVar(topicVal, top_terms),
     xlab = "1864 - Dispatch coverage", ylab = "topic saliency")
segments(x0=lda_model_topics_final$date, x1=lda_model_topics_final$date, y0=0, y1=lda_model_topics_final$freq, lty=1, lwd=2)

```

# Exploring topics

`LDAvis` offers a visual browser for topics, which has already became a very popular tool for this purpose. If everything is done right, a visualization similar to the one below should open in a browser.

![`LDAvis` Browser Example.](./LDAvis.png)

We can use the following function that extracts all needed information from a model and converts it into a format that `LDAvis` expects:

```{r}
library(LDAvis)
library(slam)

topicmodels2LDAvis <- function(x, ...){
  post <- topicmodels::posterior(x)
  if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
  mat <- x@wordassignments
  LDAvis::createJSON(
    phi = post[["terms"]], 
    theta = post[["topics"]],
    vocab = colnames(post[["terms"]]),
    doc.length = slam::row_sums(mat, na.rm = TRUE),
    term.frequency = slam::col_sums(mat, na.rm = TRUE)
  )
}
```

```{r eval=FALSE}
serVis(topicmodels2LDAvis(lda_model))
```

**NB**: there are some issues with `LDAvis` and for some reason it does not always parse out a topic model object. We can try loading another one, which does work: this is a 20 topic model based on issues of the Dispatch covering the period of 1861-1864. 

```{r eval=FALSE}
load(file="dispatch_lda_vem_better.rda")
serVis(topicmodels2LDAvis(dispatch_lda_vem_better))
```

# Additional Materials

* Chapter 3 "Analyzing word and document frequency: tf-idf" in: Silge, Julia, and David Robinson. 2017. *Text Mining with R: A Tidy Approach*. First edition. Beijing Boston Farnham: O´Reilly. <https://www.tidytextmining.com/>. Available online: <https://www.tidytextmining.com/tfidf.html>
* Chapter 6 "Topic modeling" in: Silge, Julia, and David Robinson. 2017. *Text Mining with R: A Tidy Approach*. First edition. Beijing Boston Farnham: O´Reilly. <https://www.tidytextmining.com/>. Available online: <https://www.tidytextmining.com/topicmodeling.html>
* Chapter on Topic Modeling in:  Benjamin Soltoff. *MACS 305001: Computing for the Social Sciences*, University of Chicago, <https://cfss.uchicago.edu/notes/topic-modeling/>
* David Meza. *Topic Modeling using R*, <https://knowledger.rbind.io/post/topic-modeling-using-r/>
* Grün, Bettina, and Kurt Hornik. 2011. “Topicmodels: An R Package for Fitting Topic Models.” *Journal of Statistical Software* 40 (13). <https://doi.org/10.18637/jss.v040.i13>.
